<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>OpenAI on firebirdãƒ†ã‚¯ãƒ†ã‚¯ãƒ†ã‚¯ãƒ–ãƒ­ã‚°</title>
        <link>https://firebird-techtalktech.com/tags/openai/</link>
        <description>Recent content in OpenAI on firebirdãƒ†ã‚¯ãƒ†ã‚¯ãƒ†ã‚¯ãƒ–ãƒ­ã‚°</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <copyright>ãƒˆãƒŸãƒ¼</copyright>
        <lastBuildDate>Mon, 08 Sep 2025 00:26:15 +0900</lastBuildDate><atom:link href="https://firebird-techtalktech.com/tags/openai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ğŸ“° [OpenAI] GPT-5 bio bug bounty call</title>
        <link>https://firebird-techtalktech.com/post/openai-gpt-5-bio-bug-bounty-call/</link>
        <pubDate>Mon, 08 Sep 2025 00:26:15 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/openai-gpt-5-bio-bug-bounty-call/</guid>
        <description>&lt;h2 id=&#34;æ¦‚è¦&#34;&gt;æ¦‚è¦
&lt;/h2&gt;&lt;p&gt;OpenAI invites researchers to its Bio Bug Bounty. Test GPT-5â€™s safety with a universal jailbreak prompt and win up to $25,000&amp;hellip;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ğŸ“Š &lt;strong&gt;å‡ºå…¸&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://openai.com/gpt-5-bio-bug-bounty&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenAI&lt;/a&gt;&lt;br&gt;
ğŸ“… &lt;strong&gt;åé›†æ—¥&lt;/strong&gt;: 2025å¹´09æœˆ08æ—¥&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openai.com/gpt-5-bio-bug-bounty&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ğŸ”— å…ƒè¨˜äº‹ã‚’èª­ã‚€&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ğŸ“° [OpenAI] Why language models hallucinate</title>
        <link>https://firebird-techtalktech.com/post/openai-why-language-models-hallucinate/</link>
        <pubDate>Mon, 08 Sep 2025 00:26:15 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/openai-why-language-models-hallucinate/</guid>
        <description>&lt;h2 id=&#34;æ¦‚è¦&#34;&gt;æ¦‚è¦
&lt;/h2&gt;&lt;p&gt;OpenAIâ€™s new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety&amp;hellip;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ğŸ“Š &lt;strong&gt;å‡ºå…¸&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://openai.com/index/why-language-models-hallucinate&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenAI&lt;/a&gt;&lt;br&gt;
ğŸ“… &lt;strong&gt;åé›†æ—¥&lt;/strong&gt;: 2025å¹´09æœˆ08æ—¥&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openai.com/index/why-language-models-hallucinate&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ğŸ”— å…ƒè¨˜äº‹ã‚’èª­ã‚€&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
