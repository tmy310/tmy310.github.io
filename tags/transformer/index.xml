<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on firebirdテクテクテクブログ</title>
        <link>https://firebird-techtalktech.com/tags/transformer/</link>
        <description>Recent content in Transformer on firebirdテクテクテクブログ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <copyright>トミー</copyright>
        <lastBuildDate>Sun, 14 Sep 2025 14:54:52 +0900</lastBuildDate><atom:link href="https://firebird-techtalktech.com/tags/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>📚 DPT(Dense Prediction Transformer)の論文と実装の確認</title>
        <link>https://firebird-techtalktech.com/post/dptdense-prediction-transformer%E3%81%AE%E8%AB%96%E6%96%87%E3%81%A8%E5%AE%9F%E8%A3%85%E3%81%AE%E7%A2%BA%E8%AA%8D/</link>
        <pubDate>Sun, 14 Sep 2025 14:54:52 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/dptdense-prediction-transformer%E3%81%AE%E8%AB%96%E6%96%87%E3%81%A8%E5%AE%9F%E8%A3%85%E3%81%AE%E7%A2%BA%E8%AA%8D/</guid>
        <description>&lt;h2 id=&#34;記事概要&#34;&gt;記事概要
&lt;/h2&gt;&lt;p data-sourcepos=&#34;1:1-1:192&#34;&gt;当記事ではピクセル単位での予測(Dense Predictionタスク)にViTを用いた研究であるDPT(Dense Prediction Transformer)の論文と&lt;code&gt;PyTorch&lt;/code&gt;実装の確認を行います。&lt;/p&gt;
&lt;h2 data-sourcepos=&#34;3:1-3:15&#34;&gt;
&lt;span id=&#34;dptの論文&#34; class=&#34;fragment&#34;&gt;&lt;/span&gt;&lt;a href=&#34;#dpt%E3%81%AE%E8%AB%96%E6%96%87&#34;&gt;&lt;i class=&#34;fa fa-link&#34;&gt;&lt;/i&gt;&lt;/a&gt;DPTの論文&lt;/h2&gt;
&lt;h3 data-sourcepos=&#34;4:1-4:10&#34;&gt;
&lt;span id=&#34;概要&#34; class=&#34;fragment&#34;&gt;&lt;/span&gt;&lt;a href=&#34;#%E6%A6%82%E8%A6%81&#34;&gt;&lt;i class=&#34;fa fa-link&#34;&gt;&lt;/i&gt;&lt;/a&gt;概要&lt;/h3&gt;
&lt;p data-sourcepos=&#34;5:1-6:56&#34;&gt;&lt;a href=&#34;https://qiita-us
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;出典&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://qiita.com/Chi_corp_123/items/8a2e9a4f542a3404a700&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qiita&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;収集日時&lt;/strong&gt;: 2025-09-14T14:54:15.126585&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://qiita.com/Chi_corp_123/items/8a2e9a4f542a3404a700&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;🔗 元記事を読む&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>📚 GPTをゼロから実装して理解してみる（第10部：モデルの評価と検証編）</title>
        <link>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC10%E9%83%A8%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%A9%95%E4%BE%A1%E3%81%A8%E6%A4%9C%E8%A8%BC%E7%B7%A8/</link>
        <pubDate>Sun, 14 Sep 2025 14:54:52 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC10%E9%83%A8%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%A9%95%E4%BE%A1%E3%81%A8%E6%A4%9C%E8%A8%BC%E7%B7%A8/</guid>
        <description>&lt;h2 id=&#34;記事概要&#34;&gt;記事概要
&lt;/h2&gt;&lt;blockquote data-sourcepos=&#34;1:1-1:69&#34;&gt;
&lt;p data-sourcepos=&#34;1:3-1:69&#34;&gt;&lt;em&gt;Andrej Karpathy「Let&#39;s build GPT」解説シリーズ 第4動画&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 data-sourcepos=&#34;3:1-3:16&#34;&gt;
&lt;span id=&#34;はじめに&#34; class=&#34;fragment&#34;&gt;&lt;/span&gt;&lt;a href=&#34;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB&#34;&gt;&lt;i class=&#34;fa fa-link&#34;&gt;&lt;/i&gt;&lt;/a&gt;はじめに&lt;/h3&gt;
&lt;p data-sourcepos=&#34;4:1-4:337&#34;&gt;これまでに、GPTモデルをゼロから構築し、マルチGPUでの大規模学習パイプラインを完成させました。しかし、学習中の損失（loss）が下がっているだけでは、モデルが本当に「賢く」なっているのか、汎用的な能力を獲得しているのかは分かりません。&lt;/p&gt;
&lt;p data-sourcepos=&#34;6:1-6:234&#34;&gt;この最終回では、モデルの性能を客観的
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;出典&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/c710b47d4442b57c37d1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qiita&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;収集日時&lt;/strong&gt;: 2025-09-14T14:54:15.126676&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/c710b47d4442b57c37d1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;🔗 元記事を読む&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>📚 GPTをゼロから実装して理解してみる（第9部：大規模学習へのスケールアップ編）</title>
        <link>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC9%E9%83%A8%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%AD%A6%E7%BF%92%E3%81%B8%E3%81%AE%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E7%B7%A8/</link>
        <pubDate>Sun, 14 Sep 2025 14:54:52 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC9%E9%83%A8%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%AD%A6%E7%BF%92%E3%81%B8%E3%81%AE%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E7%B7%A8/</guid>
        <description>&lt;h2 id=&#34;記事概要&#34;&gt;記事概要
&lt;/h2&gt;&lt;blockquote data-sourcepos=&#34;1:1-1:69&#34;&gt;
&lt;p data-sourcepos=&#34;1:3-1:69&#34;&gt;&lt;em&gt;Andrej Karpathy「Let&#39;s build GPT」解説シリーズ 第4動画&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 data-sourcepos=&#34;3:1-3:16&#34;&gt;
&lt;span id=&#34;はじめに&#34; class=&#34;fragment&#34;&gt;&lt;/span&gt;&lt;a href=&#34;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB&#34;&gt;&lt;i class=&#34;fa fa-link&#34;&gt;&lt;/i&gt;&lt;/a&gt;はじめに&lt;/h3&gt;
&lt;p data-sourcepos=&#34;4:1-4:375&#34;&gt;前回は、重み初期化やFlash Attention、学習率スケジューラといった高度なテクニックを導入し、単一GPUでの学習を最適化しました。しかし、現代の言語モデルは数十億〜数兆パラメータに達し、学習には膨大な計算資源が必要です。単一GPUのメモリや計算能力には限界があります。&lt;/p&gt;
&lt;p data-sourcepos=&#34;6:1-6:377
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;出典&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/eb9607d61af63db0b15e&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qiita&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;収集日時&lt;/strong&gt;: 2025-09-14T14:54:15.126683&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/eb9607d61af63db0b15e&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;🔗 元記事を読む&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
