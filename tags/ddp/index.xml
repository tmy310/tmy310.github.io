<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DDP on firebirdテクテクテクブログ</title>
        <link>https://firebird-techtalktech.com/tags/ddp/</link>
        <description>Recent content in DDP on firebirdテクテクテクブログ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja</language>
        <copyright>トミー</copyright>
        <lastBuildDate>Sun, 14 Sep 2025 14:54:52 +0900</lastBuildDate><atom:link href="https://firebird-techtalktech.com/tags/ddp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>📚 GPTをゼロから実装して理解してみる（第9部：大規模学習へのスケールアップ編）</title>
        <link>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC9%E9%83%A8%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%AD%A6%E7%BF%92%E3%81%B8%E3%81%AE%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E7%B7%A8/</link>
        <pubDate>Sun, 14 Sep 2025 14:54:52 +0900</pubDate>
        
        <guid>https://firebird-techtalktech.com/post/gpt%E3%82%92%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E7%90%86%E8%A7%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B%E7%AC%AC9%E9%83%A8%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%AD%A6%E7%BF%92%E3%81%B8%E3%81%AE%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E7%B7%A8/</guid>
        <description>&lt;h2 id=&#34;記事概要&#34;&gt;記事概要
&lt;/h2&gt;&lt;blockquote data-sourcepos=&#34;1:1-1:69&#34;&gt;
&lt;p data-sourcepos=&#34;1:3-1:69&#34;&gt;&lt;em&gt;Andrej Karpathy「Let&#39;s build GPT」解説シリーズ 第4動画&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 data-sourcepos=&#34;3:1-3:16&#34;&gt;
&lt;span id=&#34;はじめに&#34; class=&#34;fragment&#34;&gt;&lt;/span&gt;&lt;a href=&#34;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB&#34;&gt;&lt;i class=&#34;fa fa-link&#34;&gt;&lt;/i&gt;&lt;/a&gt;はじめに&lt;/h3&gt;
&lt;p data-sourcepos=&#34;4:1-4:375&#34;&gt;前回は、重み初期化やFlash Attention、学習率スケジューラといった高度なテクニックを導入し、単一GPUでの学習を最適化しました。しかし、現代の言語モデルは数十億〜数兆パラメータに達し、学習には膨大な計算資源が必要です。単一GPUのメモリや計算能力には限界があります。&lt;/p&gt;
&lt;p data-sourcepos=&#34;6:1-6:377
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;出典&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/eb9607d61af63db0b15e&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qiita&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;収集日時&lt;/strong&gt;: 2025-09-14T14:54:15.126683&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://qiita.com/keishi_irisa/items/eb9607d61af63db0b15e&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;🔗 元記事を読む&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
